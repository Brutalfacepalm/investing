{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ddfae49-5590-4f4a-9eb4-a638cd9044f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install einops, torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61784cbd-fb5c-432a-baae-1bd46ff23670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "# import math\n",
    "import numpy as np\n",
    "\n",
    "# from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# from torchsummary import summary\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, seq_len=150, patch_size=15, \n",
    "                 channels=3, #num_classes=9, \n",
    "                 latent_dim=100, embed_dim=10, \n",
    "                 depth=3, num_heads=5, \n",
    "                 forward_drop_rate=0.5, attn_drop_rate=0.5):\n",
    "        super(Generator, self).__init__()\n",
    "        self.channels = channels # признаковая размерность \n",
    "        self.latent_dim = latent_dim # размерность N шума на входе генератора\n",
    "        self.seq_len = seq_len # W или последовательность временная\n",
    "        self.embed_dim = embed_dim # пока не понятно, размерность с которой сопоставляется шум?\n",
    "        self.patch_size = patch_size # число патчей, на которые делится последовательность W чтобы получить позиционное кодирование\n",
    "        self.depth = depth # какой то гиперпараметр трансформера\n",
    "        self.num_heads = num_heads # число голов внимания в блоках трансформера\n",
    "        self.attn_drop_rate = attn_drop_rate # видимо дропаут атеншна\n",
    "        self.forward_drop_rate = forward_drop_rate # видимо дропаут прямого прохода трансформера\n",
    "        \n",
    "        # num_classes num_heads\n",
    "        \n",
    "        self.l1 = nn.Linear(self.latent_dim, self.seq_len * self.embed_dim) # преобразует длину шума N в длину временной последовательности seq_len * embed_dim, отсюда embed_dim видимо параметр, какие числом вариантом мы можем представить каждый временной шаг\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.seq_len, self.embed_dim)) # видимо обучаемый позиционный кодинг\n",
    "        self.blocks = Gen_TransformerEncoder(depth=self.depth, emb_size = self.embed_dim, num_heads=self.num_heads, drop_p=self.attn_drop_rate, forward_drop_p=self.forward_drop_rate) # непосредственно сам трансформер\n",
    "\n",
    "        self.deconv = nn.Sequential(nn.Conv2d(self.embed_dim, self.channels, 1, 1, 0)) # выходной слой, который преобразует все что нагенерировали в нужную размерность, тоесть из embed_dim получаем число каналов, но тогда в этом случае embed_dim это и есть число признаков внутри трансформера\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.l1(z).view(-1, self.seq_len, self.embed_dim)\n",
    "        x = x + self.pos_embed\n",
    "        H, W = 1, self.seq_len\n",
    "        x = self.blocks(x)\n",
    "        x = x.transpose(1, 2).unsqueeze(-2)\n",
    "        output = self.deconv(x)\n",
    "        output = output\n",
    "        return output\n",
    "    \n",
    "class Gen_TransformerEncoderBlock(nn.Sequential): # строится один блок енкодера с num_heads голов внимания\n",
    "    def __init__(self, emb_size, num_heads=5, drop_p=0.5, forward_expansion=4, forward_drop_p=0.5):\n",
    "        super().__init__(ResidualAdd(nn.Sequential(nn.LayerNorm(emb_size), MultiHeadAttention(emb_size, num_heads, drop_p), nn.Dropout(drop_p))),\n",
    "                         ResidualAdd(nn.Sequential(nn.LayerNorm(emb_size), FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p), nn.Dropout(drop_p)))\n",
    "                        )\n",
    "\n",
    "class Gen_TransformerEncoder(nn.Sequential): # последовательно встраиваемое число, равное depth, блоков енкодера \n",
    "    def __init__(self, depth=8, **kwargs):\n",
    "        super().__init__(*[Gen_TransformerEncoderBlock(**kwargs) for _ in range(depth)])       \n",
    "             \n",
    "class MultiHeadAttention(nn.Module): # блок внимания\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
    "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads) # ЕСЛИ МОЖНО надо бы заменить на чисто pytorch, чтобы уйти от библиотеки einops\n",
    "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads) # ЕСЛИ МОЖНО надо бы заменить на чисто pytorch, чтобы уйти от библиотеки einops\n",
    "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads) # ЕСЛИ МОЖНО надо бы заменить на чисто pytorch, чтобы уйти от библиотеки einops\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  # batch, num_heads, query_len, key_len # ЕСЛИ МОЖНО надо бы заменить на чисто pytorch, чтобы уйти от библиотеки einops\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "\n",
    "        scaling = self.emb_size ** (1 / 2)\n",
    "        att = F.softmax(energy / scaling, dim=-1)\n",
    "        att = self.att_drop(att)\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values) # ЕСЛИ МОЖНО надо бы заменить на чисто pytorch, чтобы уйти от библиотеки einops\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\") # ЕСЛИ МОЖНО надо бы заменить на чисто pytorch, чтобы уйти от библиотеки einops\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "class ResidualAdd(nn.Module): # резидуал блок, который строится в блоке трансформера\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "     \n",
    "class FeedForwardBlock(nn.Sequential): # сеть прямого распространения, которая строится в блоке транфсормера\n",
    "    def __init__(self, emb_size, expansion, drop_p):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "        \n",
    "class Dis_TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size=100,\n",
    "                 num_heads=5,\n",
    "                 drop_p=0.,\n",
    "                 forward_expansion=4,\n",
    "                 forward_drop_p=0.):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "\n",
    "\n",
    "class Dis_TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth=8, **kwargs):\n",
    "        super().__init__(*[Dis_TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "        \n",
    "        \n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size=100, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.clshead = nn.Sequential(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size),\n",
    "            nn.Linear(emb_size, n_classes),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.clshead(x).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class PatchEmbedding_Linear(nn.Module):\n",
    "    #what are the proper parameters set here?\n",
    "    def __init__(self, in_channels = 21, patch_size = 16, emb_size = 100, seq_len = 1024):\n",
    "        # self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        #change the conv2d parameters here\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1 = 1, s2 = patch_size),\n",
    "            nn.Linear(patch_size*in_channels, emb_size)\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((seq_len // patch_size) + 1, emb_size))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x = x.unsqueeze(2)\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
    "        #prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        # position\n",
    "        x += self.positions\n",
    "        return x        \n",
    "        \n",
    "        \n",
    "class DiscriminatorTTS(nn.Sequential):\n",
    "    def __init__(self, \n",
    "                 in_channels=3, # по аналогии с генератором - признаковая размерность\n",
    "                 patch_size=15, # по аналогии с генератором - число патчей для разбивки временной последовательности\n",
    "                 emb_size=50, # скрытое состояние\n",
    "                 seq_len=150, # длина временной последовательности на входе\n",
    "                 depth=3, # глубина самого транфсормера\n",
    "                 num_heads=5,\n",
    "                 n_classes=1, # предсказывает вероятность real/fake\n",
    "                 **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding_Linear(in_channels, patch_size, emb_size, seq_len),\n",
    "            Dis_TransformerEncoder(depth, emb_size=emb_size, num_heads=num_heads, drop_p=0.5, forward_drop_p=0.5, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3878be-b7ac-4291-b7b2-0dbcde077ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingLearned(nn.Module):\n",
    "    \"\"\"\n",
    "    Absolute pos embedding, learned.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len=27, num_pos_feats=512):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(seq_len, num_pos_feats)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.embed.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[1], x.shape[2]\n",
    "        j = torch.arange(h, device=x.device)\n",
    "        pos = self.embed(j)\n",
    "        pos = pos.repeat(x.shape[0], 1, 1)\n",
    "        return x + pos\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, device, input_size, src_seq_len, batch_first,  \n",
    "                 n_encoder_layers, n_decoder_layers, n_heads, dim_val, \n",
    "                 dropout_encoder, dropout_decoder, dropout_pos_enc,\n",
    "                 dim_feedforward_encoder, dim_feedforward_decoder,\n",
    "                 output_size, pred_len):\n",
    "        super(Discriminator, self).__init__() \n",
    "        self.output_size = output_size\n",
    "        self.device = device\n",
    "        self.input_size = input_size\n",
    "        self.src_seq_len = src_seq_len - pred_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.encoder_input_layer = nn.Linear(in_features=input_size, out_features=dim_val)\n",
    "        self.encoder_input_layer.to(self.device)\n",
    "        self.decoder_input_layer = nn.Linear(in_features=input_size, out_features=dim_val)  \n",
    "        self.decoder_input_layer.to(self.device)\n",
    "        \n",
    "        self.positional_encoding_layer = PositionEmbeddingLearned(seq_len=src_seq_len, \n",
    "                                                                  num_pos_feats=dim_val)\n",
    "        self.positional_encoding_layer.to(self.device)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_val, nhead=n_heads,\n",
    "                                                   dim_feedforward=dim_feedforward_encoder,\n",
    "                                                   dropout=dropout_encoder, \n",
    "                                                   batch_first=batch_first, \n",
    "                                                   activation='relu', \n",
    "                                                   norm_first=True\n",
    "                                                   )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, \n",
    "                                             num_layers=n_encoder_layers,\n",
    "                                             norm=None\n",
    "                                             )\n",
    "        self.encoder.to(self.device)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=dim_val, nhead=n_heads,\n",
    "                                                   dim_feedforward=dim_feedforward_decoder,\n",
    "                                                   dropout=dropout_decoder, \n",
    "                                                   batch_first=batch_first, \n",
    "                                                   activation='relu', \n",
    "                                                   norm_first=True\n",
    "                                                   )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, \n",
    "                                             num_layers=n_decoder_layers,\n",
    "                                             norm=None\n",
    "                                             )\n",
    "        self.decoder.to(self.device)\n",
    "        \n",
    "        self.decoder_output_layer = nn.Sequential(torch.nn.Linear(in_features=dim_val, out_features=output_size),\n",
    "                                                  # torch.nn.Sigmoid()\n",
    "                                                  )\n",
    "        \n",
    "        # self.src_mask = torch.triu(torch.ones((self.pred_len + 1, self.src_seq_len), \n",
    "        #                                       device=self.device) * float('-inf'), diagonal=1).to(torch.bool)\n",
    "        # self.tgt_mask = torch.triu(torch.ones((self.pred_len + 1, self.pred_len + 1), \n",
    "        #                                       device=self.device) * float('-inf'), diagonal=1).to(torch.bool)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        tgt = src[:, -1, :].clone().unsqueeze(1)\n",
    "        # src = src[:, :, :]\n",
    "\n",
    "        encoder_output = self.encoder_input_layer(src) \n",
    "        encoder_output = self.positional_encoding_layer(encoder_output) \n",
    "        encoder_output = self.encoder(encoder_output)\n",
    "\n",
    "        decoder_output = self.decoder_input_layer(tgt)\n",
    "        decoder_output = self.decoder(tgt=decoder_output, memory=encoder_output, tgt_mask=None, memory_mask=None)\n",
    "\n",
    "        out = self.decoder_output_layer(decoder_output)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d61c466-3a5c-4f44-8af5-d106b89a1d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "G_tts 17039625, D_tts 5869441, D 2550529, G_tts/D_tts 2.90, G_tts/D 6.68\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "gen = Generator(seq_len = 471, \n",
    "                patch_size = 471, \n",
    "                channels = 9, \n",
    "                latent_dim = 100, \n",
    "                embed_dim = 256, \n",
    "                depth = 6, \n",
    "                num_heads = 32)\n",
    "gen_numel = sum([i.numel() for i in gen.parameters()])\n",
    "dis_tts = DiscriminatorTTS(in_channels = 9, \n",
    "                    patch_size = 157, \n",
    "                    emb_size = 384, \n",
    "                    seq_len = 471, \n",
    "                    depth = 3, \n",
    "                    num_heads = 8)\n",
    "distts_numel = sum([i.numel() for i in dis_tts.parameters()])\n",
    "dis = Discriminator(device, \n",
    "                    input_size=9, \n",
    "                    src_seq_len=471,\n",
    "                    batch_first=True, \n",
    "                    n_encoder_layers=8, \n",
    "                    n_decoder_layers=8, \n",
    "                    n_heads=8, \n",
    "                    dim_val=64, \n",
    "                    dropout_encoder=0.2,\n",
    "                    dropout_decoder=0.2, \n",
    "                    dropout_pos_enc=0.1,\n",
    "                    dim_feedforward_encoder=1024, \n",
    "                    dim_feedforward_decoder=1024, \n",
    "                    output_size=1, \n",
    "                    pred_len=1)\n",
    "dis_numel = sum([i.numel() for i in dis.parameters()])\n",
    "print(f'G_tts {gen_numel}, D_tts {distts_numel}, D {dis_numel}, G_tts/D_tts {(gen_numel / distts_numel):.2f}, G_tts/D {(gen_numel / dis_numel):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0eedaf5b-4bb6-4a72-a6d2-1cd4216a943f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 1, 471])\n",
      "torch.Size([1, 471, 9])\n",
      "torch.Size([1])\n",
      "tensor([0.0837], grad_fn=<ViewBackward0>)\n",
      "CPU times: user 4.08 s, sys: 2.92 s, total: 7.01 s\n",
      "Wall time: 989 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# z = torch.FloatTensor(2, 100).normal_(0, 1)\n",
    "z = torch.FloatTensor(np.random.normal(0, 1, (1, 100)))\n",
    "g_out = gen(z)\n",
    "print(g_out.shape)\n",
    "print(g_out.squeeze(2).transpose(1,2).shape)\n",
    "d_out = dis(g_out.squeeze(2).transpose(1,2)).view(-1)\n",
    "print(d_out.shape)\n",
    "print(d_out)\n",
    "\n",
    "# real_seq = torch.FloatTensor(1, 54, 557).normal_(0, 1)\n",
    "# print(real_seq.shape)\n",
    "# d_out = dis(real_seq.transpose(1, 2).unsqueeze(2))\n",
    "# print(d_out.shape)\n",
    "# print(d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfc2f3-9fe6-451a-b37f-013dc9c6d145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9254f-5859-4d4e-b0b6-9d146bde34e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = self.l1(z).view(-1, self.seq_len, self.embed_dim)\n",
    "x = x + self.pos_embed\n",
    "H, W = 1, self.seq_len\n",
    "x = self.blocks(x)\n",
    "print(x.shape)\n",
    "x = x.transpose(1, 2).unsqueeze(-1)\n",
    "# x = x.reshape(x.shape[0], x.shape[1], 1, x.shape[2])\n",
    "# print(x.shape)\n",
    "# x = x.permute(0, 3, 1, 2)\n",
    "print(x.shape)\n",
    "output = self.deconv(x)\n",
    "print(output.shape)\n",
    "output = output.view(-1, self.channels, H, W)\n",
    "print(output.shape)\n",
    "return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57114f59-69af-4968-ba84-c850b7ff9943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    }
   ],
   "source": [
    "count_params = 0\n",
    "params_to_update = []\n",
    "\n",
    "for param in dis.parameters():\n",
    "    params_to_update.append(param)\n",
    "print(len(params_to_update))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c85c4-4301-4f54-9f53-4c31848f181f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c108eb-850e-49d5-9133-2549d374967f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb7e2c-ff77-4dbc-b5d5-aba632d90415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0201a-cde4-453f-83fe-dc16001c44ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6003a0-15ea-47b1-95b0-964a7f8ef128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f76fef-e8f2-403e-a856-e5a6e1b21d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e04c8-c8a4-41a6-ab4f-ac7b46ddfffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a01dc7-169d-4a6f-b571-cae485b53449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b555e-7cb4-4e3f-999d-b67aadd6b192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0c004-3e53-4e15-9cd3-1ea63917b92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb5fa6-8a1b-4b4d-95be-339e604f87c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56cbef5-4cde-42b4-b396-decda2487580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b134406-0e2c-4bf5-a6f6-97c47c72d216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
